{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated one-dimensional latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import combinations\n",
    "from gpytorch.kernels import RQKernel\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(seed, p=5):\n",
    "\n",
    "    N = 2 ** p\n",
    "    torch.random.manual_seed(seed)\n",
    "    W = (torch.randn(p, 1) - 0.2) * 2\n",
    "\n",
    "    X = torch.zeros(N, p)\n",
    "    ind = 1\n",
    "\n",
    "    # for all # of mutations\n",
    "    for mutations in range(1, p + 1):\n",
    "\n",
    "        # for selected combination of mutations for a variant\n",
    "        for variant in combinations(range(p), mutations):\n",
    "\n",
    "            # for each selected\n",
    "            for s in variant:\n",
    "                X[ind, s] = 1\n",
    "\n",
    "            # update after variant\n",
    "            ind += 1\n",
    "\n",
    "    z = torch.mm(X, W)\n",
    "    Z = torch.linspace(z.min(), z.max(), 100)[:, None]\n",
    "    z_samp = torch.cat((z, Z), 0)\n",
    "\n",
    "    kernel = RQKernel()\n",
    "    with torch.no_grad():\n",
    "        K = kernel(z_samp).evaluate()\n",
    "        f = torch.distributions.MultivariateNormal(\n",
    "            torch.zeros(N + 100), 0.0025 * K + torch.eye(N + 100) * 1e-7\n",
    "        ).rsample() + torch.sigmoid(0.9+ z_samp[:, 0])\n",
    "        \n",
    "    y = f[:N] + torch.randn(N) * 0.05\n",
    "\n",
    "    return W, X, z, y, Z, f[N:]\n",
    "\n",
    "p = 5\n",
    "W, X, z, y, Z, f = sim(100, p=p)\n",
    "\n",
    "plt.figure(figsize=(4, 3), dpi=300)\n",
    "plt.plot(Z, f)\n",
    "plt.scatter(z, y, c=\"C2\", alpha=0.8)\n",
    "plt.axvline(0, c=\"k\", ls=\"--\")\n",
    "\n",
    "for i in range(p):\n",
    "    plt.arrow(0, -.05*i, W[i].item(), 0, color=f\"C{3+i}\", width=0.01)\n",
    "    \n",
    "plt.ylabel(\"phenotype\")\n",
    "plt.xlabel(\"$z_1$\")\n",
    "    \n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the generated dataset to a pandas DataFrame for use with LANTERN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"substitutions\": [\n",
    "            \":\".join(\n",
    "                [\n",
    "                    # encode each mutation as one of +a, +b, ...\n",
    "                    \"+{}\".format(string.ascii_lowercase[i])\n",
    "                    for i in np.where(X[j, :].numpy())[0]\n",
    "                ]\n",
    "            )\n",
    "            for j in range(X.shape[0])\n",
    "        ],\n",
    "        \"phenotype\": ((y-y.mean())/y.std()).numpy(),\n",
    "    },\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LANTERN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lantern.dataset import Dataset\n",
    "ds = Dataset(df)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32 observations\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first element (a tuple of x_0, y_0)\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lantern.model.basis import VariationalBasis\n",
    "\n",
    "basis = VariationalBasis.fromDataset(ds, K=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lantern.model.surface import Phenotype\n",
    "\n",
    "surface = Phenotype.fromDataset(ds, K=K, Ni=200, inducScale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lantern.model import Model\n",
    "\n",
    "model = Model(basis, surface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "loss = model.loss(N=len(ds))\n",
    "Xtrain, ytrain = ds[: len(ds)]\n",
    "\n",
    "E = 5000\n",
    "optimizer = Adam(loss.parameters(), lr=0.01)\n",
    "hist = []\n",
    "halpha = np.zeros((E, K))\n",
    "\n",
    "for i in range(E):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    yhat = model(Xtrain)\n",
    "    lss = loss(yhat, ytrain)\n",
    "    total = sum(lss.values())\n",
    "    total.backward()\n",
    "    optimizer.step()\n",
    "    # print(basis.log_alpha.grad.abs().max())\n",
    "    \n",
    "    hist.append(total.item())\n",
    "    halpha[i, :] = basis.qalpha(detach=True).mean.numpy()\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(4, 3), dpi=300)\n",
    "plt.plot(hist)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also review the learned variance for each dimension at each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3), dpi=300)\n",
    "plt.plot(1/halpha)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"variance\")\n",
    "plt.semilogy()\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = 1/basis.qalpha(detach=True).mean[basis.order]\n",
    "\n",
    "plt.figure(figsize=(4, 3), dpi=300)\n",
    "plt.plot(qa)\n",
    "plt.scatter(range(K), qa)\n",
    "\n",
    "plt.xticks(range(K))\n",
    "plt.semilogy()\n",
    "plt.xlabel(\"dimensions\")\n",
    "plt.ylabel(\"variance\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of dimensions by relevance (e.g. their variance) is stochastic, and can change on different re-runs. We provide an `order` attribute on the `basis` component to address this problem. The `order` is the indexes of the latent dimensions sorted by their variance. We store the index of the highest relevance dimension here for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = basis.order[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compared learned latent effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wapprox = basis.W_mu[:, z1].detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(4, 3), dpi=300)\n",
    "plt.scatter(Wapprox, W)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot learned surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the learned surface to the true, underlying surface. Since LANTERN is unaware of the underlying scale of the latent mutational effect dimension, we rescale the true $z_1$ to match that of the $z_1$ learned by LANTERN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    Zapprox = basis(X)\n",
    "    \n",
    "    Zpred = torch.zeros(100, K)\n",
    "    Zpred[:, z1] = torch.linspace(Zapprox[:, z1].min(), Zapprox[:, z1].max(), 100)\n",
    "\n",
    "    fpred = surface(Zpred)\n",
    "    \n",
    "    lo, hi = fpred.confidence_region()\n",
    "    \n",
    "plt.figure(figsize=(4, 3), dpi=300)\n",
    "\n",
    "plt.plot(Zpred[:, z1].numpy(), fpred.mean.numpy())\n",
    "plt.fill_between(Zpred[:, z1].numpy(), lo.numpy(), hi.numpy(), alpha=0.6)\n",
    "\n",
    "scale = Zapprox[:, z1].mean() / z.mean()\n",
    "plt.plot(Z*scale, (f - y.mean())/y.std())\n",
    "plt.scatter(z*scale, (y - y.mean())/y.std(), c=\"C2\", alpha=0.8)\n",
    "plt.axvline(0, c=\"k\", ls=\"--\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
